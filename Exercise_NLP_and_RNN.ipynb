{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Assignment4_NLP_and_RNN_exercise_CASI_DONE.ipynb","provenance":[],"collapsed_sections":["A9PiYvfro0TA"],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"cells":[{"cell_type":"markdown","metadata":{"id":"Yq5dzMnlU5g4"},"source":["# NLP and RNN\n","In this notebook we will check our understanding of the concepts learned in NLP and RNN.\n","\n","![](https://drive.google.com/uc?export=view&id=1ZtBJcNXO-BQhK86joluZYzN7b1GHYLLs)\n","\n","To summarize NLP or Natural Language Processing is:\n","\n","* Computer manipulation of natural languages.\n","* Set of methods/algorithms for making natural language accessible to computer.\n","\n","\n","The image below summarizes the basic steps involved in any NLP task:\n","\n","![](https://drive.google.com/uc?export=view&id=1PNXHplPzlWnrddeh5oPX6ZuJSfgnuHCa)\n","\n","\n","\n","There are 5 exercises in total and an optional exercise. To answer some of the exercises (3, 4 and 5) you will be required to write a code from scratch in the code cells containing:\n","\n","`# write your code here`\n","\n","\n","Before starting make sure you are using the GPU."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"J0nYAP8Gg8Rn","executionInfo":{"status":"ok","timestamp":1616348057884,"user_tz":180,"elapsed":1303,"user":{"displayName":"Oscar Patricio Ordenes Gallegos","photoUrl":"","userId":"06495825101710856750"}},"outputId":"dd9232cd-595f-4b88-e9c5-3fbfa9276f48"},"source":["!nvidia-smi"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Sun Mar 21 17:34:15 2021       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.56       Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   41C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"tFW1f15r0-Nj"},"source":["## Tokenization\n","\n","\n","We will use TensorFlow Keras Tokenizer to tokenize our text. As per the [TensorFlow documentation](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer):\n","\n","“This class allows to vectorize a text corpus, by turning each text into either a sequence of integers (each integer being the index of a token in a dictionary) or into a vector where the coefficient for each token could be binary, based on word count, based on tf-idf.”\n","\n","\n","There are many functions that we can use, but below we will be using these two functions to train the tokenizer to our text data and convert given text to tokens:\n","\n","* `fit_on_texts`: Updates internal vocabulary based on a list of texts. This method creates the vocabulary index based on word frequency. So if you give it something like, \"The cat sat on the mat.\" It will create a dictionary “word_index” such that every word gets a unique integer value. 0 is reserved for padding. So lower integer means more frequent word.\n","\n","* `texts_to_sequences` Transforms each text in texts to a sequence of integers. It takes each word in the text and replaces it with its corresponding integer value from the word_index dictionary. \n","\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NIlcRTXsQ1tu","executionInfo":{"status":"ok","timestamp":1619318255336,"user_tz":240,"elapsed":4077,"user":{"displayName":"Oscar Patricio Ordenes Gallegos","photoUrl":"","userId":"06495825101710856750"}},"outputId":"47cff22e-9283-46d9-8edc-91a53d3c4ec9"},"source":["from tensorflow.keras.preprocessing.text import Tokenizer\n","t  = Tokenizer()\n","fit_text = [\"In science, you can say things that seem crazy, but in the long run, they can turn out to be right\"]\n","t.fit_on_texts(fit_text)\n","\n","\n","test_text1 = \"I would like to take a right turn\"\n","test_text2 = \"That man is crazy\"\n","sequences = t.texts_to_sequences([test_text1, test_text2])\n","\n","print('sequences : ',sequences,'\\n')\n","\n","print('word_index : ',t.word_index)"],"execution_count":1,"outputs":[{"output_type":"stream","text":["sequences :  [[17, 19, 15], [7, 9]] \n","\n","word_index :  {'in': 1, 'can': 2, 'science': 3, 'you': 4, 'say': 5, 'things': 6, 'that': 7, 'seem': 8, 'crazy': 9, 'but': 10, 'the': 11, 'long': 12, 'run': 13, 'they': 14, 'turn': 15, 'out': 16, 'to': 17, 'be': 18, 'right': 19}\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"pvdb0TqFQ8fm"},"source":["#### Exercise 1\n","In the code above we tokenize two sentences:\n","* \"I would like to take a right turn\"\n","* \"That man is crazy\"\n","\n","a. What is the tokenized version of these sentences?\n","\n","\n","b. The first sentence has 8 words, and second sentence has 4 words, however the tokenized version has 3 and 2 integers respectively for them. Why is it so?"]},{"cell_type":"markdown","metadata":{"id":"oCd4YxPfTZwz"},"source":["**Answer 1**: \n","\n","A. [\"I\",\"would\",\"like\",\"to\",\"take\",\"a\",\"right\",\"turn\"] and [\"That\",\"man\",\"is\",\"crazy\"]"]},{"cell_type":"markdown","metadata":{"id":"NOC-X3I4SyDo"},"source":["## Embeddings\n"]},{"cell_type":"markdown","metadata":{"id":"JI3bQ3Xkn9JQ"},"source":["#### Exercise 2\n","In the class we learned about embeddings, let us explore them a little more. Kindly go to the site [Embeddings Projector](http://projector.tensorflow.org). Play around a bit and answer the following questions:\n","\n","1.  For the word 'fantastic' list the five nearest neighbours, when using `Word2Vec 10K` embedding.\n","2. Repeat the exercise by changing the embeddings to `Word2Vec All`.\n","\n","Reflect on the result. How do you think the world `fantastic` is related to its five nearest neighbours?"]},{"cell_type":"markdown","metadata":{"id":"miPD6Wu1WPPZ"},"source":["**Answer 2**: \n","\n","1. In descending order: marvel, spider, amazing, strange, weird, comics, daredevil, storyline\n","\n","2. Fantastic, Fantastical, Fantastically\n","\n","\n","Although they do not have a direct relationship in terms of meaning, these words can be related according to the objective of the sentence. That is, they could, a sentence could have hinted that daredevil is fantastic, for example. In question 2. The results are directly related to the word \"fantastic\".\n"]},{"cell_type":"markdown","metadata":{"id":"MRtE9HduWWoo"},"source":["## Word Similarities\n","Let us now train Word2Vec model on text8 dataset"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HSlDQfKoR5zs","executionInfo":{"status":"ok","timestamp":1619318482817,"user_tz":240,"elapsed":140244,"user":{"displayName":"Oscar Patricio Ordenes Gallegos","photoUrl":"","userId":"06495825101710856750"}},"outputId":"d2fadfd3-7b65-4074-e6a2-0a65cccc2c66"},"source":["!mkdir data\n","\n","import gensim.downloader as api\n","from gensim.models import Word2Vec\n","\n","info = api.info(\"text8\")\n","assert(len(info) > 0)\n","\n","dataset = api.load(\"text8\")  # download and load text 8  dataset\n","model = Word2Vec(dataset) # we create an embedding using Word2vec model for this data\n","\n","model.save(\"data/text8-word2vec.bin\")"],"execution_count":2,"outputs":[{"output_type":"stream","text":["[==================================================] 100.0% 31.6/31.6MB downloaded\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"j4qqaBk7XLGu"},"source":["#### Load the saved model as KeyedVector to save space."]},{"cell_type":"code","metadata":{"id":"W26H--UnW3cL","executionInfo":{"status":"ok","timestamp":1619318483589,"user_tz":240,"elapsed":108909,"user":{"displayName":"Oscar Patricio Ordenes Gallegos","photoUrl":"","userId":"06495825101710856750"}}},"source":["from gensim.models import KeyedVectors\n","model = KeyedVectors.load(\"data/text8-word2vec.bin\")  # Help in saving memory by shedding the internal data structures necessary for training\n","word_vectors = model.wv   ## Gives the word vectors"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Fj9Y7PsAXJjG"},"source":["#### Helper function to print"]},{"cell_type":"code","metadata":{"id":"E7Tku-zOXGK8","executionInfo":{"status":"ok","timestamp":1619318483591,"user_tz":240,"elapsed":101547,"user":{"displayName":"Oscar Patricio Ordenes Gallegos","photoUrl":"","userId":"06495825101710856750"}}},"source":["def print_most_similar(word_conf_pairs, k):\n","    for i, (word, conf) in enumerate(word_conf_pairs):\n","        print(\"{:.3f} {:s}\".format(conf, word))\n","        if i >= k-1:\n","            break\n","    if k < len(word_conf_pairs):\n","        print(\"...\")"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RUJ2ZigzXvH9","executionInfo":{"status":"ok","timestamp":1619318483593,"user_tz":240,"elapsed":101047,"user":{"displayName":"Oscar Patricio Ordenes Gallegos","photoUrl":"","userId":"06495825101710856750"}},"outputId":"72390189-2b03-4a7d-ad64-535ab1202849"},"source":["print_most_similar(word_vectors.most_similar('king'),5)"],"execution_count":5,"outputs":[{"output_type":"stream","text":["0.725 prince\n","0.719 emperor\n","0.695 queen\n","0.694 throne\n","0.687 vii\n","...\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"kq-ugPPeW5IJ"},"source":["#### Exercise 3\n","In the class, we learned how to use the Word2Vec embeddings in Gensim. When the model is trained on the ‘text8’ dataset, give five most similar words to the word ‘tree’ using word2vec embedding trained on ‘text8’ dataset."]},{"cell_type":"code","metadata":{"id":"AbQnXKWFW98W","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619318492148,"user_tz":240,"elapsed":688,"user":{"displayName":"Oscar Patricio Ordenes Gallegos","photoUrl":"","userId":"06495825101710856750"}},"outputId":"7b401ddb-e4af-477a-c8cc-49f9358fe28d"},"source":["## Write your code here\n","\n","print_most_similar(word_vectors.most_similar('tree'),5)"],"execution_count":6,"outputs":[{"output_type":"stream","text":["0.701 trees\n","0.686 leaf\n","0.661 bark\n","0.649 vine\n","0.644 fruit\n","...\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"zAjhxGApXji3"},"source":["**Answer 3**:\n","\n","trees, bark, leaf, avl, fruit"]},{"cell_type":"markdown","metadata":{"id":"zVcvHhtkZCyU"},"source":["## Word Arithmetics\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"yKI_ojISoRJw"},"source":["#### Exercise 4\n","With the Word2Vec model trained on text8 dataset, calculate the following:\n","\n","*\twoman + king - man = ?\n","*\tchair + table - work = ?\n","*\tQueens - queen + person = ?"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M35vzMIRZY0C","executionInfo":{"status":"ok","timestamp":1619318507586,"user_tz":240,"elapsed":708,"user":{"displayName":"Oscar Patricio Ordenes Gallegos","photoUrl":"","userId":"06495825101710856750"}},"outputId":"5cfaaa2e-d4f5-445e-a9b6-260c8b8b8392"},"source":["## Write your code here\n","result = model.most_similar(positive=['woman', 'king'], negative=['man'], topn=1);\n","print(result);\n","\n","result = model.most_similar(positive=['chair', 'table'], negative=['work'], topn=1);\n","print(result);\n","\n","result = model.most_similar(positive=['queens', 'person'], negative=['queen'], topn=1);\n","print(result);"],"execution_count":7,"outputs":[{"output_type":"stream","text":["[('queen', 0.6665854454040527)]\n","[('bar', 0.6440656185150146)]\n","[('node', 0.5654397010803223)]\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n","  \n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n","  \"\"\"\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n","  \n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"_zLpG_LcZT4T"},"source":["**Answer 4**: (Double Click to edit)\n","\n","*\twoman + king - man = queen\n","*\tchair + table - work = bar\n","*\tQueens - queen + person = finite"]},{"cell_type":"markdown","metadata":{"id":"bBBqBcUDZxMt"},"source":["## Spam Classifier"]},{"cell_type":"markdown","metadata":{"id":"lXlZhHnAlM7K"},"source":["Some helper codes:\n","* importing required modules\n","* defining helper functions\n","* Building model\n","\n","The code cells below are  hidden, that is by default you cannot see the code in them, but remember to run these cells. You can check the code by double clicking the cells."]},{"cell_type":"code","metadata":{"id":"SWUieaCOZRx_","executionInfo":{"status":"ok","timestamp":1619318518167,"user_tz":240,"elapsed":673,"user":{"displayName":"Oscar Patricio Ordenes Gallegos","photoUrl":"","userId":"06495825101710856750"}}},"source":["#@title\n","# The modules needed to run the code\n","import argparse  # To read commandline argument and parse it\n","import gensim.downloader as api\n","import numpy as np\n","import os  # For file and directory handling\n","import shutil  # For file and directory handling\n","import tensorflow as tf\n","\n","from sklearn.metrics import accuracy_score, confusion_matrix  #For measuring performance\n","\n","# Some parameters\n","DATA_DIR = \"data\"   # Data directory to save embedding\n","EMBEDDING_NUMPY_FILE = os.path.join(DATA_DIR, \"E.npy\")  # Numpy file containing word embeddings\n","DATASET_URL = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip\"  # Dataset URL from where data is downloaded\n","EMBEDDING_MODEL = \"glove-wiki-gigaword-300\"  # The gensim embedding model we will use\n","EMBEDDING_DIM = 300  # The embedding dimensions\n","NUM_CLASSES = 2  # The number of classes in output-- Spam or Ham\n","BATCH_SIZE = 128  # The batch size\n","NUM_EPOCHS = 3  # number of epochs for which model is to be trained\n","\n","\n","# data distribution is 4827 ham and 747 spam (total 5574), which \n","# works out to approx 87% ham and 13% spam, so we take reciprocals\n","# and this works out to being each spam (1) item as being approximately\n","# 8 times as important as each ham (0) message.\n","CLASS_WEIGHTS = { 0: 1, 1: 8 }  # To take care of imbalance in classes\n","\n","tf.random.set_seed(42)  # Set the seed for random number generation to be able to reproduce results. \n","\n","\n","# Data downloading and data Processing\n","\n","\n","\n","def download_and_read(url):\n","    \"\"\"\n","    The function downloads the data from given url, splits it into Text and Labels\n","    Uses tf.keras.utils.get_file() function to download the data from url--> function \n","    downloads the data from the given url, extracts it from the zip file and place it in folder \"datasets\" \n","    with the name specified in the first argument.\n","    tf.keras.utils.get_file(\n","    fname, origin, untar=False, md5_hash=None, file_hash=None,\n","    cache_subdir='datasets', hash_algorithm='auto', extract=False,\n","    archive_format='auto', cache_dir=None)\n","\n","    Arguments:\n","    url: The url link of the dataset in zip format\n","\n","    Returns:\n","    Two lists containing texts and respective labels\n","\n","    \"\"\"\n","    local_file = url.split('/')[-1]  # split the file name (last string after '/') from url\n","    p = tf.keras.utils.get_file(local_file, url, \n","        extract=True, cache_dir=\".\")  #function to download the data from url to folder datasets with name given in local_file\n","    labels, texts = [], []\n","    local_file = os.path.join(\"datasets\", \"SMSSpamCollection\")  # define the path of the file from which to read data: datasets/SMSSpamCollection\n","    with open(local_file, \"r\") as fin:\n","        for line in fin:\n","            label, text = line.strip().split('\\t')  # The labels and text are in one line separated by tab space.\n","            labels.append(1 if label == \"spam\" else 0)\n","            texts.append(text)\n","    return texts, labels\n","\n","def build_embedding_matrix(sequences, word2idx, embedding_dim, \n","        embedding_file):\n","    \"\"\"\n","    The function reads the dict word2idx (word --> number) and written the corresponding\n","    word vector for each word as defined by the Embedding model\n","\n","    Arguments:\n","    sequences: not needed, not used-- just there because to suport back support for TF1 book\n","    word2idx: Dictionary  containing words in the text and their respective idx as given by tokenizer.\n","    embedding_dim: The number of units for the embedding layer\n","    embedding_file: The data file in which embeddings will be store for future use.\n","\n","    \"\"\"\n","    if os.path.exists(embedding_file):  # Checks if the embedding file already exists- then it justs loads it in the memory\n","        E = np.load(embedding_file)\n","    else:  # Else it creates the embedding file using the model specified in EMBEDDING_MODEL\n","        vocab_size = len(word2idx)  # The vocabulary size is number of unique words in the text\n","        E = np.zeros((vocab_size, embedding_dim)) # Creates a variable to store embeddings\n","        word_vectors = api.load(EMBEDDING_MODEL)  # Get the embeddings from Gensim\n","        for word, idx in word2idx.items():\n","            try:\n","                E[idx] = word_vectors.word_vec(word)  # For each word it converts it to respective word vector and store in Embedding file\n","            except KeyError:   # word not in embedding\n","                pass\n","            # except IndexError: # UNKs are mapped to seq over VOCAB_SIZE as well as 1\n","            #     pass\n","        np.save(embedding_file, E)  # The embeddings are saved in a file for future reference\n","    return E"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"sCiIzfo5Zv4J","executionInfo":{"status":"ok","timestamp":1619318521142,"user_tz":240,"elapsed":674,"user":{"displayName":"Oscar Patricio Ordenes Gallegos","photoUrl":"","userId":"06495825101710856750"}}},"source":["#@title\n","class SpamClassifierModel(tf.keras.Model):  # The model is build using model API of Keras with tf.Keras.Model as the parent class. \n","# The class inherits train, predict methods of the parent class.\n","    def __init__(self, vocab_sz, embed_sz, input_length,\n","            num_filters, kernel_sz, output_sz, \n","            run_mode, embedding_weights, \n","            **kwargs):\n","        super(SpamClassifierModel, self).__init__(**kwargs)\n","        if run_mode == \"scratch\":  # Choose the embedding layer scratch means the weights wil be traned from scratch\n","            self.embedding = tf.keras.layers.Embedding(vocab_sz, \n","                embed_sz,\n","                input_length=input_length,\n","                trainable=True)\n","        elif run_mode == \"vectorizer\":  # Vectorizer means we use the pre-trained weights--> Transfer Learning\n","            self.embedding = tf.keras.layers.Embedding(vocab_sz, \n","                embed_sz,\n","                input_length=input_length,\n","                weights=[embedding_weights],\n","                trainable=False)\n","        else:  # This is the fine tuning mode- we use pre-trained weights for the embedding layer and fine tune them. \n","            self.embedding = tf.keras.layers.Embedding(vocab_sz, \n","                embed_sz,\n","                input_length=input_length,\n","                weights=[embedding_weights],\n","                trainable=True)\n","        self.dropout = tf.keras.layers.SpatialDropout1D(0.2)  # Add droput layer to avoid overfotting. \n","        self.conv = tf.keras.layers.Conv1D(filters=num_filters,  # Define the 1D convolutional layer \n","            kernel_size=kernel_sz,\n","            activation=\"relu\")\n","        self.pool = tf.keras.layers.GlobalMaxPooling1D()  # The pooling layer\n","        self.dense = tf.keras.layers.Dense(output_sz, \n","            activation=\"softmax\")  # And the last classifying layer consists of a fully connected Dense layer\n","\n","    def call(self, x):  # This function performs forward pass in the model. \n","        x = self.embedding(x)\n","        x = self.dropout(x)\n","        x = self.conv(x)\n","        x = self.pool(x)\n","        x = self.dense(x)\n","        return x"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yMNs8evikv8z","executionInfo":{"status":"ok","timestamp":1619318779018,"user_tz":240,"elapsed":229188,"user":{"displayName":"Oscar Patricio Ordenes Gallegos","photoUrl":"","userId":"06495825101710856750"}},"outputId":"b8e93c95-0e4d-4a1a-e6b8-769eb6ebb0d9"},"source":["#@title\n","# The code below requires a folder to be created\n","!mkdir data\n","\n","## Now we will use the functions and model defined above --> ideally they should be done in a separate file-- main.py\n","\n","# read data\n","texts, labels = download_and_read(DATASET_URL)\n","\n","# tokenize and pad text so that each text is of same size\n","tokenizer = tf.keras.preprocessing.text.Tokenizer()\n","tokenizer.fit_on_texts(texts)\n","text_sequences = tokenizer.texts_to_sequences(texts)\n","text_sequences = tf.keras.preprocessing.sequence.pad_sequences(text_sequences)\n","num_records = len(text_sequences)\n","max_seqlen = len(text_sequences[0])\n","#print(\"{:d} sentences, max length: {:d}\".format(num_records, max_seqlen))\n","\n","# labels --> convert labels to categorical labels (one hot encoded)\n","cat_labels = tf.keras.utils.to_categorical(labels, num_classes=NUM_CLASSES)\n","\n","# vocabulary --> Create word mapping and its inverse\n","word2idx = tokenizer.word_index\n","idx2word = {v:k for k, v in word2idx.items()}\n","word2idx[\"PAD\"] = 0\n","idx2word[0] = \"PAD\"\n","vocab_size = len(word2idx)\n","#print(\"vocab size: {:d}\".format(vocab_size))\n","\n","# load the dataset as tensors, split it into test, train and validation set\n","dataset = tf.data.Dataset.from_tensor_slices((text_sequences, cat_labels))\n","dataset = dataset.shuffle(10000)\n","test_size = num_records // 4\n","val_size = (num_records - test_size) // 10\n","test_dataset = dataset.take(test_size)\n","val_dataset = dataset.skip(test_size).take(val_size)\n","train_dataset = dataset.skip(test_size + val_size)\n","\n","test_dataset = test_dataset.batch(BATCH_SIZE, drop_remainder=True)\n","val_dataset = val_dataset.batch(BATCH_SIZE, drop_remainder=True)\n","train_dataset = train_dataset.batch(BATCH_SIZE, drop_remainder=True)\n","\n","# Build the embedding\n","E = build_embedding_matrix(text_sequences, word2idx, EMBEDDING_DIM,\n","    EMBEDDING_NUMPY_FILE)\n","#print(\"Embedding matrix:\", E.shape)\n","\n","#Since we are not passing the mode by command line in this file we need to give a value to run_mode\n","run_mode = 'scratch'\n"],"execution_count":10,"outputs":[{"output_type":"stream","text":["mkdir: cannot create directory ‘data’: File exists\n","Downloading data from https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip\n","204800/203415 [==============================] - 0s 1us/step\n","[==================================================] 100.0% 376.1/376.1MB downloaded\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"MndykezKk56_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619318803284,"user_tz":240,"elapsed":780,"user":{"displayName":"Oscar Patricio Ordenes Gallegos","photoUrl":"","userId":"06495825101710856750"}},"outputId":"6b044eb1-a29b-40f8-ec33-d221bb03fa59"},"source":["# Now we use the SpamClassifierModel class to create a model\n","conv_num_filters = 256\n","conv_kernel_size = 3\n","model = SpamClassifierModel(\n","    vocab_size, EMBEDDING_DIM, max_seqlen, \n","    conv_num_filters, conv_kernel_size, NUM_CLASSES,\n","    run_mode, E)\n","model.build(input_shape=(None, max_seqlen))\n","model.summary()"],"execution_count":11,"outputs":[{"output_type":"stream","text":["Model: \"spam_classifier_model\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding (Embedding)        multiple                  2703000   \n","_________________________________________________________________\n","spatial_dropout1d (SpatialDr multiple                  0         \n","_________________________________________________________________\n","conv1d (Conv1D)              multiple                  230656    \n","_________________________________________________________________\n","global_max_pooling1d (Global multiple                  0         \n","_________________________________________________________________\n","dense (Dense)                multiple                  514       \n","=================================================================\n","Total params: 2,934,170\n","Trainable params: 2,934,170\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"VLeHPvIumV6q"},"source":["#### Compile and train the model"]},{"cell_type":"code","metadata":{"id":"DWtsovxqmVAV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619318847051,"user_tz":240,"elapsed":38068,"user":{"displayName":"Oscar Patricio Ordenes Gallegos","photoUrl":"","userId":"06495825101710856750"}},"outputId":"537fe1ff-1430-4748-af71-3ff6e0476487"},"source":["# Define  compile and train\n","model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\",\n","    metrics=[\"accuracy\"])\n","\n","# Now we train the model\n","model.fit(train_dataset, epochs=NUM_EPOCHS, \n","    validation_data=val_dataset,\n","    class_weight=CLASS_WEIGHTS)"],"execution_count":12,"outputs":[{"output_type":"stream","text":["Epoch 1/3\n","29/29 [==============================] - 34s 64ms/step - loss: 1.1398 - accuracy: 0.4778 - val_loss: 0.2144 - val_accuracy: 0.9896\n","Epoch 2/3\n","29/29 [==============================] - 1s 48ms/step - loss: 0.3875 - accuracy: 0.9693 - val_loss: 0.0552 - val_accuracy: 0.9792\n","Epoch 3/3\n","29/29 [==============================] - 1s 48ms/step - loss: 0.0850 - accuracy: 0.9913 - val_loss: 0.0539 - val_accuracy: 0.9818\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x7f4b9ea1f090>"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"markdown","metadata":{"id":"1Cc0hUMTnAX9"},"source":["### And now we evaluate the model on test dataset."]},{"cell_type":"code","metadata":{"id":"rr1dT-1UnBFn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619318864260,"user_tz":240,"elapsed":631,"user":{"displayName":"Oscar Patricio Ordenes Gallegos","photoUrl":"","userId":"06495825101710856750"}},"outputId":"9876c15f-8c8a-4e2b-9e14-ad500b80dfc9"},"source":["# Lastly we evaluate the trained model against test set\n","labels, predictions = [], []\n","for Xtest, Ytest in test_dataset:  \n","    Ytest_ = model.predict_on_batch(Xtest)  # for each test test predict the label\n","    ytest = np.argmax(Ytest, axis=1)  # Get the label with highest probabilty from actual test output\n","    ytest_ = np.argmax(Ytest_, axis=1) # Get the label with highest probabilty from predictted test output\n","    labels.extend(ytest.tolist())  # add to list\n","    predictions.extend(ytest.tolist())  # add to list\n","\n","print(\"test accuracy: {:.3f}\".format(accuracy_score(labels, predictions)))  # Calculate accuracy score"],"execution_count":13,"outputs":[{"output_type":"stream","text":["test accuracy: 1.000\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"jEah9aBlneEf"},"source":["#### Exercise 5\n","In the spam classifier what is the false positive and false negative on the test dataset?  What does it tell you about the trained model?"]},{"cell_type":"code","metadata":{"id":"Pj69HAaZnToi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619318938090,"user_tz":240,"elapsed":674,"user":{"displayName":"Oscar Patricio Ordenes Gallegos","photoUrl":"","userId":"06495825101710856750"}},"outputId":"389e6c41-1af5-4e3b-b97e-03cec4f61881"},"source":["# Write your code here\n","import numpy as np\n","from sklearn.metrics import confusion_matrix\n","\n","print(np.unique(labels))\n","print(np.unique(predictions))\n","\n","print(confusion_matrix(labels, predictions))\n","\n"],"execution_count":14,"outputs":[{"output_type":"stream","text":["[0 1]\n","[0 1]\n","[[1091    0]\n"," [   0  189]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"rBxQkUtv3E6U"},"source":["**Answer 5**: \n","False positive and false negative are zero (0). According to the test accuracy, the model classifies all the instances correctly.  While this may be positive, this could also be a sign of overfitting."]},{"cell_type":"markdown","metadata":{"id":"A9PiYvfro0TA"},"source":["# Optional Exercise\n","Consider the Tweet sentiment dataset from Stanford: http://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip Which fields you will require for training? Modify the `download_and_read(url)` function to read from this URL and return the necessary labeled sentences.\n","\n","What is the accuracy of your twitter sentiment model after 5 epochs, after 10 epochs, after 15 epochs? Do you think it should be trained for more than 15 epochs? Give reasoning for your answer. \n","\n","In the class we used bi-LSTM, try change the recurrent network from bi-LSTM to GRU, how does it change the model performance?"]},{"cell_type":"code","metadata":{"id":"KyWTIIu9Amvz"},"source":["#@title\n","# The modules needed to run the code\n","import argparse  # To read commandline argument and parse it\n","import gensim.downloader as api\n","import numpy as np\n","import os  # For file and directory handling\n","import shutil  # For file and directory handling\n","import tensorflow as tf\n","\n","from sklearn.metrics import accuracy_score, confusion_matrix  #For measuring performance\n","\n","# Some parameters\n","DATA_DIR = \"data_2\"   # Data directory to save embedding\n","EMBEDDING_NUMPY_FILE = os.path.join(DATA_DIR, \"E.npy\")  # Numpy file containing word embeddings\n","DATASET_URL = \"http://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip\"  # Dataset URL from where data is downloaded\n","EMBEDDING_MODEL = \"glove-wiki-gigaword-300\"  # The gensim embedding model we will use\n","EMBEDDING_DIM = 300  # The embedding dimensions\n","NUM_CLASSES = 2  # The number of classes in output-- Spam or Ham\n","BATCH_SIZE = 128  # The batch size\n","NUM_EPOCHS = 3  # number of epochs for which model is to be trained\n","\n","\n","# data distribution is 4827 ham and 747 spam (total 5574), which \n","# works out to approx 87% ham and 13% spam, so we take reciprocals\n","# and this works out to being each spam (1) item as being approximately\n","# 8 times as important as each ham (0) message.\n","CLASS_WEIGHTS = { 0: 1, 1: 8 }  # To take care of imbalance in classes\n","\n","tf.random.set_seed(42)  # Set the seed for random number generation to be able to reproduce results. \n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8Lm6mBMA9WXu"},"source":["#@title\n","# The code below requires a folder to be created\n","!mkdir data_2\n","\n","## Now we will use the functions and model defined above --> ideally they should be done in a separate file-- main.py\n","\n","# read data\n","texts, labels = download_and_read(DATASET_URL)\n","\n","# tokenize and pad text so that each text is of same size\n","tokenizer = tf.keras.preprocessing.text.Tokenizer()\n","tokenizer.fit_on_texts(texts)\n","text_sequences = tokenizer.texts_to_sequences(texts)\n","text_sequences = tf.keras.preprocessing.sequence.pad_sequences(text_sequences)\n","num_records = len(text_sequences)\n","max_seqlen = len(text_sequences[0])\n","#print(\"{:d} sentences, max length: {:d}\".format(num_records, max_seqlen))\n","\n","# labels --> convert labels to categorical labels (one hot encoded)\n","cat_labels = tf.keras.utils.to_categorical(labels, num_classes=NUM_CLASSES)\n","\n","# vocabulary --> Create word mapping and its inverse\n","word2idx = tokenizer.word_index\n","idx2word = {v:k for k, v in word2idx.items()}\n","word2idx[\"PAD\"] = 0\n","idx2word[0] = \"PAD\"\n","vocab_size = len(word2idx)\n","#print(\"vocab size: {:d}\".format(vocab_size))\n","\n","# load the dataset as tensors, split it into test, train and validation set\n","dataset = tf.data.Dataset.from_tensor_slices((text_sequences, cat_labels))\n","dataset = dataset.shuffle(10000)\n","test_size = num_records // 4\n","val_size = (num_records - test_size) // 10\n","test_dataset = dataset.take(test_size)\n","val_dataset = dataset.skip(test_size).take(val_size)\n","train_dataset = dataset.skip(test_size + val_size)\n","\n","test_dataset = test_dataset.batch(BATCH_SIZE, drop_remainder=True)\n","val_dataset = val_dataset.batch(BATCH_SIZE, drop_remainder=True)\n","train_dataset = train_dataset.batch(BATCH_SIZE, drop_remainder=True)\n","\n","# Build the embedding\n","E = build_embedding_matrix(text_sequences, word2idx, EMBEDDING_DIM,\n","    EMBEDDING_NUMPY_FILE)\n","#print(\"Embedding matrix:\", E.shape)\n","\n","#Since we are not passing the mode by command line in this file we need to give a value to run_mode\n","run_mode = 'scratch'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Hcg4OKDp_DQF","executionInfo":{"status":"ok","timestamp":1616350881285,"user_tz":180,"elapsed":711,"user":{"displayName":"Oscar Patricio Ordenes Gallegos","photoUrl":"","userId":"06495825101710856750"}},"outputId":"3d880ff8-308e-45aa-a591-d282d3ad205a"},"source":["# using the same model\n","conv_num_filters = 256\n","conv_kernel_size = 3\n","model = SpamClassifierModel(\n","    vocab_size, EMBEDDING_DIM, max_seqlen, \n","    conv_num_filters, conv_kernel_size, NUM_CLASSES,\n","    run_mode, E)\n","model.build(input_shape=(None, max_seqlen))\n","model.summary()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"spam_classifier_model_3\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding_3 (Embedding)      multiple                  2703000   \n","_________________________________________________________________\n","spatial_dropout1d_3 (Spatial multiple                  0         \n","_________________________________________________________________\n","conv1d_3 (Conv1D)            multiple                  230656    \n","_________________________________________________________________\n","global_max_pooling1d_3 (Glob multiple                  0         \n","_________________________________________________________________\n","dense_3 (Dense)              multiple                  514       \n","=================================================================\n","Total params: 2,934,170\n","Trainable params: 2,934,170\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ub2-ygXq_H5q"},"source":["# Define  compile and train\n","model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\",\n","    metrics=[\"accuracy\"])\n","\n","NUM_EPOCHS = [5,10,15]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s64E_8-l_LUZ","executionInfo":{"status":"ok","timestamp":1616350947639,"user_tz":180,"elapsed":44226,"user":{"displayName":"Oscar Patricio Ordenes Gallegos","photoUrl":"","userId":"06495825101710856750"}},"outputId":"db987b97-8720-4ba7-8920-0ee12415d0c3"},"source":["# Now we train the model\n","\n","for i in range(0,len(NUM_EPOCHS)):\n","  model.fit(train_dataset, epochs=NUM_EPOCHS[i], \n","  validation_data=val_dataset,\n","  class_weight=CLASS_WEIGHTS)\n","  \n","  labels, predictions = [], []\n","  for Xtest, Ytest in test_dataset:  \n","    Ytest_ = model.predict_on_batch(Xtest)  # for each test test predict the label\n","    ytest = np.argmax(Ytest, axis=1)  # Get the label with highest probabilty from actual test output\n","    ytest_ = np.argmax(Ytest_, axis=1) # Get the label with highest probabilty from predictted test output\n","    labels.extend(ytest.tolist())  # add to list\n","    predictions.extend(ytest.tolist())  # add to list\n","  print(\"test accuracy: {:.3f}\".format(accuracy_score(labels, predictions)))  # Calculate accuracy score"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/5\n","29/29 [==============================] - 2s 61ms/step - loss: 1.1398 - accuracy: 0.4778 - val_loss: 0.2144 - val_accuracy: 0.9896\n","Epoch 2/5\n","29/29 [==============================] - 1s 50ms/step - loss: 0.3875 - accuracy: 0.9693 - val_loss: 0.0552 - val_accuracy: 0.9792\n","Epoch 3/5\n","29/29 [==============================] - 1s 48ms/step - loss: 0.0850 - accuracy: 0.9913 - val_loss: 0.0539 - val_accuracy: 0.9818\n","Epoch 4/5\n","29/29 [==============================] - 1s 48ms/step - loss: 0.0631 - accuracy: 0.9911 - val_loss: 0.0142 - val_accuracy: 0.9974\n","Epoch 5/5\n","29/29 [==============================] - 1s 46ms/step - loss: 0.0323 - accuracy: 0.9980 - val_loss: 0.0119 - val_accuracy: 1.0000\n","test accuracy: 1.000\n","test accuracy: 1.000\n","test accuracy: 1.000\n","test accuracy: 1.000\n","test accuracy: 1.000\n","test accuracy: 1.000\n","test accuracy: 1.000\n","test accuracy: 1.000\n","test accuracy: 1.000\n","test accuracy: 1.000\n","Epoch 1/10\n","29/29 [==============================] - 1s 48ms/step - loss: 0.0143 - accuracy: 0.9992 - val_loss: 0.0078 - val_accuracy: 1.0000\n","Epoch 2/10\n","29/29 [==============================] - 1s 46ms/step - loss: 0.0094 - accuracy: 0.9995 - val_loss: 0.0031 - val_accuracy: 1.0000\n","Epoch 3/10\n","29/29 [==============================] - 1s 48ms/step - loss: 0.0054 - accuracy: 0.9997 - val_loss: 0.0021 - val_accuracy: 1.0000\n","Epoch 4/10\n","29/29 [==============================] - 1s 47ms/step - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.0011 - val_accuracy: 1.0000\n","Epoch 5/10\n","29/29 [==============================] - 1s 47ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.0010 - val_accuracy: 1.0000\n","Epoch 6/10\n","29/29 [==============================] - 1s 46ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 7.9425e-04 - val_accuracy: 1.0000\n","Epoch 7/10\n","29/29 [==============================] - 1s 48ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 6.7383e-04 - val_accuracy: 1.0000\n","Epoch 8/10\n","29/29 [==============================] - 1s 46ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 3.8044e-04 - val_accuracy: 1.0000\n","Epoch 9/10\n","29/29 [==============================] - 1s 47ms/step - loss: 9.5922e-04 - accuracy: 1.0000 - val_loss: 4.5901e-04 - val_accuracy: 1.0000\n","Epoch 10/10\n","29/29 [==============================] - 1s 47ms/step - loss: 9.1870e-04 - accuracy: 1.0000 - val_loss: 3.9855e-04 - val_accuracy: 1.0000\n","test accuracy: 1.000\n","test accuracy: 1.000\n","test accuracy: 1.000\n","test accuracy: 1.000\n","test accuracy: 1.000\n","test accuracy: 1.000\n","test accuracy: 1.000\n","test accuracy: 1.000\n","test accuracy: 1.000\n","test accuracy: 1.000\n","Epoch 1/15\n","29/29 [==============================] - 1s 49ms/step - loss: 6.7212e-04 - accuracy: 1.0000 - val_loss: 3.9746e-04 - val_accuracy: 1.0000\n","Epoch 2/15\n","29/29 [==============================] - 1s 50ms/step - loss: 6.1701e-04 - accuracy: 1.0000 - val_loss: 2.2471e-04 - val_accuracy: 1.0000\n","Epoch 3/15\n","29/29 [==============================] - 1s 49ms/step - loss: 5.6161e-04 - accuracy: 1.0000 - val_loss: 3.1396e-04 - val_accuracy: 1.0000\n","Epoch 4/15\n","29/29 [==============================] - 1s 48ms/step - loss: 4.8844e-04 - accuracy: 1.0000 - val_loss: 3.2114e-04 - val_accuracy: 1.0000\n","Epoch 5/15\n","29/29 [==============================] - 1s 48ms/step - loss: 5.0463e-04 - accuracy: 1.0000 - val_loss: 2.4292e-04 - val_accuracy: 1.0000\n","Epoch 6/15\n","29/29 [==============================] - 1s 47ms/step - loss: 4.2854e-04 - accuracy: 1.0000 - val_loss: 2.5748e-04 - val_accuracy: 1.0000\n","Epoch 7/15\n","29/29 [==============================] - 1s 47ms/step - loss: 3.4258e-04 - accuracy: 1.0000 - val_loss: 1.2351e-04 - val_accuracy: 1.0000\n","Epoch 8/15\n","29/29 [==============================] - 1s 48ms/step - loss: 3.3593e-04 - accuracy: 1.0000 - val_loss: 2.2178e-04 - val_accuracy: 1.0000\n","Epoch 9/15\n","29/29 [==============================] - 1s 48ms/step - loss: 3.3052e-04 - accuracy: 1.0000 - val_loss: 2.2068e-04 - val_accuracy: 1.0000\n","Epoch 10/15\n","29/29 [==============================] - 1s 47ms/step - loss: 3.0021e-04 - accuracy: 1.0000 - val_loss: 1.3649e-04 - val_accuracy: 1.0000\n","Epoch 11/15\n","29/29 [==============================] - 1s 46ms/step - loss: 2.3923e-04 - accuracy: 1.0000 - val_loss: 1.3781e-04 - val_accuracy: 1.0000\n","Epoch 12/15\n","29/29 [==============================] - 1s 48ms/step - loss: 2.1555e-04 - accuracy: 1.0000 - val_loss: 1.1820e-04 - val_accuracy: 1.0000\n","Epoch 13/15\n","29/29 [==============================] - 1s 48ms/step - loss: 1.9350e-04 - accuracy: 1.0000 - val_loss: 1.3517e-04 - val_accuracy: 1.0000\n","Epoch 14/15\n","29/29 [==============================] - 1s 48ms/step - loss: 2.1707e-04 - accuracy: 1.0000 - val_loss: 8.6352e-05 - val_accuracy: 1.0000\n","Epoch 15/15\n","29/29 [==============================] - 1s 47ms/step - loss: 1.8153e-04 - accuracy: 1.0000 - val_loss: 7.9565e-05 - val_accuracy: 1.0000\n","test accuracy: 1.000\n","test accuracy: 1.000\n","test accuracy: 1.000\n","test accuracy: 1.000\n","test accuracy: 1.000\n","test accuracy: 1.000\n","test accuracy: 1.000\n","test accuracy: 1.000\n","test accuracy: 1.000\n","test accuracy: 1.000\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"gntSuKdM_5tq"},"source":[""],"execution_count":null,"outputs":[]}]}